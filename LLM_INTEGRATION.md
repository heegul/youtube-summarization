# Local LLM Integration Guide

This document outlines how to integrate local Large Language Models (LLMs) using Ollama into the YouTube Summarization application for tag generation and summarization.

## Overview

The YouTube Summarization application can use both cloud-based LLMs (like GPT-4) and local LLMs (via Ollama) for generating summaries and extracting tags. Local LLMs provide several advantages:

- **Privacy**: All processing happens locally, with no data sent to external services
- **Cost**: No usage fees for inference
- **Customization**: Ability to fine-tune models for specific summarization styles
- **Offline capability**: Summaries can be generated without internet connectivity

## Setup and Requirements

### Prerequisites

- Ollama installed ([Download here](https://ollama.ai/))
- Minimum 16GB RAM for smaller models, 32GB+ recommended for larger ones
- SSD with at least 20GB free space
- CUDA-compatible GPU recommended but not required

### Installing Ollama

```bash
# macOS or Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Verify installation
ollama --version
```

### Pulling Required Models

```bash
# Pull Llama 2 (7B parameters)
ollama pull llama2

# Pull Mistral (7B parameters)
ollama pull mistral

# Pull a specialized model for summarization (optional)
ollama pull llama2:13b
```

## Ollama Integration

### OllamaService Implementation

Create `backend/services/ollamaService.ts` for interfacing with Ollama API, with methods for:
- Getting available models
- Generating summaries
- Extracting tags from summaries
- Checking Ollama availability

### LLM Factory Pattern

Create `backend/services/llmFactory.ts` to select appropriate LLM implementation:
- Select between local (Ollama) and cloud (OpenAI) providers
- Implement fallback mechanisms when local LLM is unavailable
- Handle model-specific configurations

## API Implementation

### Model Types

Define types in `backend/types/llm.ts`:
- LLM Provider types (local or cloud)
- LLM Model interface with metadata
- Available models configuration

### REST Endpoints

Add to `backend/routes/videoRoutes.ts`:
- `GET /api/videos/llm/models` - List available LLM models
- Enhanced `GET /api/videos/summary/:videoId` - Support model selection

## Frontend Integration

### Model Selector Component

Create `frontend/components/ModelSelector.tsx`:
- Dropdown for selecting LLM models
- Display model provider (local vs. cloud)
- Handle loading states and errors

### Usage in Home Page

Update main page to:
- Include model selector
- Pass model selection to API calls
- Display tags generated by LLM

## Performance Considerations

### Optimizing Local LLM Performance

1. **Model Quantization**
   - Use quantized versions of models for faster inference

2. **GPU Acceleration**
   - Ensure CUDA is properly set up for Nvidia GPUs
   - For macOS, ensure Metal API is used on Apple Silicon

3. **Caching**
   - Implement caching for LLM responses to prevent redundant processing

4. **Graceful Fallback**
   - Implement fallback to cloud models if local processing fails

## Troubleshooting

### Common Issues and Solutions

1. **Ollama Connection Refused**
   - Ensure Ollama server is running (`ollama serve`)
   - Check if the port is blocked by firewall

2. **Out of Memory Errors**
   - Use smaller or quantized models
   - Increase swap space
   - Limit concurrent requests to Ollama

3. **Slow Response Times**
   - Implement request timeouts
   - Use a smaller context window
   - Consider upgrading hardware

### Monitoring and Logging

Implement comprehensive logging to track:
- LLM response times
- Success/failure rates
- Memory usage and performance metrics